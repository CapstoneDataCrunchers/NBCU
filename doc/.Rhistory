##gfg corpus
gfg.srt <- file.path("../data/script/gfg")
gfg <- VCorpus(DirSource(gfg.srt,encoding ="latin1"))
gfg
##yshr
yshr.srt <- file.path("../data/script/yshr")
yshr <- VCorpus(DirSource(yshr.srt,encoding ="latin1"))
yshr
##rhonyc corpus
rhonyc.srt <- file.path("../data/script/rhonyc")
rhonyc_txt <- VCorpus(DirSource(rhonyc.srt,encoding ="latin1"))
rhonyc_txt
##gfg corpus
gfg.srt <- file.path("../data/script/gfg")
gfg_txt <- VCorpus(DirSource(gfg.srt,encoding ="latin1"))
gfg_txt
##yshr
yshr.srt <- file.path("../data/script/yshr")
yshr_txt <- VCorpus(DirSource(yshr.srt,encoding ="latin1"))
yshr_txt
##rhonyc corpus
rhonyc.srt <- file.path("../data/script/rhonyc")
rhonyc_txt <- VCorpus(DirSource(rhonyc.srt,encoding ="latin1"))
rhonyc_txt
##gfg corpus
gfg.srt <- file.path("../data/script/gfg")
gfg_txt <- VCorpus(DirSource(gfg.srt,encoding ="latin1"))
gfg_txt
##yshr
yshr.srt <- file.path("../data/script/yshr")
yshr_txt <- VCorpus(DirSource(yshr.srt,encoding ="latin1"))
yshr_txt
acqTag.rhonyc <- tagPOS(rhonyc_txt)
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
list(POStagged = POStagged, POStags = POStags)
}
extractPOS <- function(x, thisPOSregex) {
x <- as.String(x)
wordAnnotation <- annotate(x, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
POSAnnotation <- annotate(x, Maxent_POS_Tag_Annotator(), wordAnnotation)
POSwords <- subset(POSAnnotation, type == "word")
tags <- sapply(POSwords$features, '[[', "POS")
thisPOSindex <- grep(thisPOSregex, tags)
tokenizedAndTagged <- sprintf("%s/%s", x[POSwords][thisPOSindex], tags[thisPOSindex])
untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
untokenizedAndTagged
}
acqTag.rhonyc <- tagPOS(rhonyc_txt)
acqTag.rhonyc
acqTag.rhonyc
yshr <- tm_map(yshr_txt, removePunctuation)
yshr <- tm_map(yshr, removeNumbers)
yshr <- tm_map(yshr, tolower)
yshr <- tm_map(yshr, removeWords, stopwords("english"))
yshr <- tm_map(yshr, stripWhitespace)
yshr <- tm_map(yshr, PlainTextDocument)
acqTag.yshr <- tagPOS(yshr)
acqTag.yshr
yshr_NN <- lapply(yshr, extractPOS, "NN")
yshr_NN
yshr_NN
yshr_noun <- lapply(yshr_NN, function(d) {
as.data.frame(lapply(d, gsub, pattern = "^(.*?)/.*", replacement = "\\1"))
})
yshr_noun
setwd("~/Github/NLP-Team2/doc")
packages.used=c("SnowballC", "ggplot2", "rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr", "tm", "syuzhet",
"factoextra", "scales", "RColorBrewer", "RANN", "tm",
"topicmodels","NLP","openNLP","magrittr","wordcloud",
"tidytext","stringr","data.table","shiny","XML","RCurl")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(SnowballC)
library(dplyr)
library(rvest)
library(tibble)
library(qdap)
library(sentimentr)
library(gplots)
library(ggplot2)
library(syuzhet)
library(factoextra)
library(scales)
library(RColorBrewer)
library(RANN)
library(tm)
library(topicmodels)
library(NLP)
library(openNLP)
library(magrittr)
library(wordcloud)
library(tidytext)
library(stringr)
library(shiny)
library(data.table)
library(XML)
library(RCurl)
##rhonyc corpus
rhonyc.srt <- file.path("../data/script/rhonyc")
rhonyc_txt <- VCorpus(DirSource(rhonyc.srt,encoding ="latin1"))
rhonyc_txt
##gfg corpus
gfg.srt <- file.path("../data/script/gfg")
gfg_txt <- VCorpus(DirSource(gfg.srt,encoding ="latin1"))
gfg_txt
##yshr
yshr.srt <- file.path("../data/script/yshr")
yshr_txt <- VCorpus(DirSource(yshr.srt,encoding ="latin1"))
yshr_txt
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
list(POStagged = POStagged, POStags = POStags)
}
extractPOS <- function(x, thisPOSregex) {
x <- as.String(x)
wordAnnotation <- annotate(x, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
POSAnnotation <- annotate(x, Maxent_POS_Tag_Annotator(), wordAnnotation)
POSwords <- subset(POSAnnotation, type == "word")
tags <- sapply(POSwords$features, '[[', "POS")
thisPOSindex <- grep(thisPOSregex, tags)
tokenizedAndTagged <- sprintf("%s/%s", x[POSwords][thisPOSindex], tags[thisPOSindex])
untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
untokenizedAndTagged
}
yshr <- tm_map(yshr_txt, removePunctuation)
yshr <- tm_map(yshr, removeNumbers)
yshr <- tm_map(yshr, tolower)
yshr <- tm_map(yshr, removeWords, stopwords("english"))
yshr <- tm_map(yshr, stripWhitespace)
yshr <- tm_map(yshr, PlainTextDocument)
nouns = c("/NN", "/NNS","/NNP","/NNPS")
verbs = c("/VB","/VBD","/VBG","/VBN","/VBP","/VBZ")
s <- yshr
s = tolower(s)
s <- yshr[1]
s = tolower(s)
s = gsub("\n","",s)
s = gsub('"',"",s)
s
s <- yshr[1]
s = tolower(s)
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
nouns_present
verbs_present
s <- yshr
s = tolower(s)
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
nouns_present
summary(nouns_present)
verbs_present
count(nouns_present)
get_key <- function(x) {
s <- tm_map(x, removePunctuation)
s <- tm_map(s, removeNumbers)
s <- tm_map(s, tolower)
s <- tm_map(s, removeWords, stopwords("english"))
s <- tm_map(s, stemDocument)
s <- tm_map(s, stripWhitespace)
s <- tm_map(s, PlainTextDocument)
nouns = c("/NN", "/NNS","/NNP","/NNPS")
verbs = c("/VB","/VBD","/VBG","/VBN","/VBP","/VBZ")
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
list(noun = nouns_present, verb = verbs_present)
}
gfg_key <- get_key(gfg_txt)
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
list(POStagged = POStagged, POStags = POStags)
}
extractPOS <- function(x, thisPOSregex) {
x <- as.String(x)
wordAnnotation <- annotate(x, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
POSAnnotation <- annotate(x, Maxent_POS_Tag_Annotator(), wordAnnotation)
POSwords <- subset(POSAnnotation, type == "word")
tags <- sapply(POSwords$features, '[[', "POS")
thisPOSindex <- grep(thisPOSregex, tags)
tokenizedAndTagged <- sprintf("%s/%s", x[POSwords][thisPOSindex], tags[thisPOSindex])
untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
untokenizedAndTagged
}
gfg_key <- get_key(gfg_txt)
gfg_key
get_key <- function(x) {
s <- tm_map(x, removePunctuation)
s <- tm_map(s, removeNumbers)
s <- tm_map(s, tolower)
s <- tm_map(s, removeWords, stopwords("english"))
#s <- tm_map(s, stemDocument)
s <- tm_map(s, stripWhitespace)
s <- tm_map(s, PlainTextDocument)
nouns = c("/NN", "/NNS","/NNP","/NNPS")
verbs = c("/VB","/VBD","/VBG","/VBN","/VBP","/VBZ")
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
list(noun = nouns_present, verb = verbs_present)
}
gfg_key <- get_key(gfg_txt)
gfg_key
tagPOS <-  function(x, ...) {
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s/%s", s[a3w], POStags), collapse = " ")
list(POStagged = POStagged, POStags = POStags)
}
extractPOS <- function(x, thisPOSregex) {
x <- as.String(x)
wordAnnotation <- annotate(x, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
POSAnnotation <- annotate(x, Maxent_POS_Tag_Annotator(), wordAnnotation)
POSwords <- subset(POSAnnotation, type == "word")
tags <- sapply(POSwords$features, '[[', "POS")
thisPOSindex <- grep(thisPOSregex, tags)
tokenizedAndTagged <- sprintf("%s/%s", x[POSwords][thisPOSindex], tags[thisPOSindex])
untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
untokenizedAndTagged
}
#acqTag.rhonyc <- tagPOS(rhonyc_txt)
#acqTag.gfg <- tagPOS(gfg_txt)
#acqTag.yshr <- tagPOS(yshr)
#rhonyc_NN <- lapply(rhonyc, extractPOS, "NN")
#gfg_NN <- lapply(gfg, extractPOS, "NN")
#yshr_NN <- lapply(yshr, extractPOS, "NN")
#### extract noun & verb
get_key <- function(x) {
s <- tm_map(x, removePunctuation)
s <- tm_map(s, removeNumbers)
s <- tm_map(s, tolower)
s <- tm_map(s, removeWords, stopwords("english"))
#s <- tm_map(s, stemDocument)
s <- tm_map(s, stripWhitespace)
s <- tm_map(s, PlainTextDocument)
nouns = c("/NN", "/NNS","/NNP","/NNPS")
verbs = c("/VB","/VBD","/VBG","/VBN","/VBP","/VBZ")
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
list(noun = nouns_present, verb = verbs_present)
}
rhonyc_key <- get_key(rhonyc_txt)
gfg_key <- get_key(gfg_txt)
yshr_key <- get_key(yshr_txt)
rhonyc_key
gfg_key
yshr_key
library(openNLP)
library(NLP)
s = as.String("The boy opened the box. He took the chocolates. He ate the
chocolates. He went to school")
tagPOS<-  function(x, ...) {
s <- as.String(x)
word_token_annotator<- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
POStags<- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged<- paste(sprintf("%s/%s", s[a3w], POStags), collapse = ",")
list(POStagged = POStagged, POStags = POStags)
}
nouns = c("/NN", "/NNS","/NNP","/NNPS")
verbs = c("/VB","/VBD","/VBG","/VBN","/VBP","/VBZ")
s = tolower(s)
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
nouns_present
rhonyc_key
summary(rhonyc_key)
rhonyc_n <- strsplit(rhonyc_key[1], split, fixed = FALSE, perl = FALSE, useBytes = FALSE)
rhonyc_n <- strsplit(rhonyc_key[1], split)
rhonyc_n <- strsplit(rhonyc_key[1], split = ",")
rhonyc_n <- strsplit(as.character(rhonyc_key[1]), split = ",")
rhonyc_n
no_k <- gsub("'", " ", rhonyc_key[1])
no_k
no_y <- gsub(" ", "", rhonyc_key[1])
no_y
no_y <- gsub(" ", "", no_k)
no_y
rhonyc_n <- strsplit(as.character(no_y), split = ",")
rhonyc_n
no_k <- gsub("'", " ", rhonyc_key)
no_y <- gsub(" ", "", no_k)
rhonyc_n <- strsplit(as.character(no_y), split = ",")
rhonyc_n
get_key <- function(x) {
s <- tm_map(x, removePunctuation)
s <- tm_map(s, removeNumbers)
s <- tm_map(s, tolower)
s <- tm_map(s, removeWords, stopwords("english"))
#s <- tm_map(s, stemDocument)
s <- tm_map(s, stripWhitespace)
s <- tm_map(s, PlainTextDocument)
nouns = c("/NN", "/NNS","/NNP","/NNPS")
verbs = c("/VB","/VBD","/VBG","/VBN","/VBP","/VBZ")
s = gsub("\n","",s)
s = gsub('"',"",s)
tags = tagPOS(s)
tags = tags$POStagged
tags = unlist(strsplit(tags, split=","))
nouns_present = tags[grepl(paste(nouns, collapse = "|"), tags)]
nouns_present = unique(nouns_present)
verbs_present = tags[grepl(paste(verbs, collapse = "|"), tags)]
verbs_present = unique(verbs_present)
nouns_present<- gsub("^(.*?)/.*", "\\1", nouns_present)
verbs_present = gsub("^(.*?)/.*", "\\1", verbs_present)
nouns_present =
paste("'",as.character(nouns_present),"'",collapse=",",sep="")
verbs_present =
paste("'",as.character(verbs_present),"'",collapse=",",sep="")
l <- list(noun = nouns_present, verb = verbs_present)
l <- gsub("'", " ", l)
l <- gsub(" ", "", l)
l <- strsplit(as.character(l), split = ",")
}
gfg_key <- get_key(gfg_txt)
gfg_key
yshr_key <- get_key(yshr_txt)
rhonyc_key <- get_key(rhonyc_txt)
yshr_key
tdm.yshrk <- TermDocumentMatrix(yshr_key)
tdm.tidyyshrk=tidy(tdm.yshrk)
tdm.overallyshrk=summarise(group_by(tdm.tidyyshrk, term), sum(count))
#generate the wordcloud
wordcloud(tdm.overallyshrk$term, tdm.overallyshrk$`sum(count)`, max.words=70, random.order=FALSE, random.color=FALSE,rot.per=0, colors=brewer.pal(5,"Greens"))
tdm.yshrk <- TermDocumentMatrix(yshr_key)
tdm.tidyyshrk=tidy(tdm.yshrk)
tdm.overallyshrk=summarise(group_by(tdm.tidyyshrk, term), sum(count))
#generate the wordcloud
wordcloud(tdm.overallyshrk$term, tdm.overallyshrk$`sum(count)`, max.words=70, random.order=FALSE, random.color=FALSE,rot.per=0, colors=brewer.pal(5,"Greens"))
tdm.yshrk <- TermDocumentMatrix(yshr_key)
tdm.yshrk <- TermDocumentMatrix(yshr_key[1])
yshrk <- Corpus(VectorSource(yshr_key))
tdm.yshrk <- TermDocumentMatrix(yshrk)
tdm.tidyyshrk=tidy(tdm.yshrk)
tdm.overallyshrk=summarise(group_by(tdm.tidyyshrk, term), sum(count))
#generate the wordcloud
wordcloud(tdm.overallyshrk$term, tdm.overallyshrk$`sum(count)`, max.words=70, random.order=FALSE, random.color=FALSE,rot.per=0, colors=brewer.pal(5,"Greens"))
yshrk <- Corpus(VectorSource(yshr_key))
tdm.yshrk <- TermDocumentMatrix(yshrk)
tdm.tidyyshrk=tidy(tdm.yshrk)
tdm.overallyshrk=summarise(group_by(tdm.tidyyshrk, term), sum(count))
#generate the wordcloud
wordcloud(tdm.overallyshrk$term, tdm.overallyshrk$`sum(count)`, max.words=70, random.order=FALSE, random.color=FALSE,rot.per=0, colors=brewer.pal(5,"Greens"))
setwd("~/Github/NLP-Team2/doc")
packages.used=c("SnowballC", "ggplot2", "rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr", "tm", "syuzhet",
"factoextra", "scales", "RColorBrewer", "RANN", "tm",
"topicmodels","NLP","openNLP","magrittr","wordcloud",
"tidytext","stringr","data.table","shiny","XML","RCurl")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(SnowballC)
library(dplyr)
library(rvest)
library(tibble)
library(qdap)
library(sentimentr)
library(gplots)
library(ggplot2)
library(syuzhet)
library(factoextra)
library(scales)
library(RColorBrewer)
library(RANN)
library(tm)
library(topicmodels)
library(NLP)
library(openNLP)
library(magrittr)
library(wordcloud)
library(tidytext)
library(stringr)
library(shiny)
library(data.table)
library(XML)
library(RCurl)
print(R.version)
##rhonyc corpus
rhonyc.srt <- file.path("../data/script/rhonyc")
rhonyc_txt <- VCorpus(DirSource(rhonyc.srt,encoding ="latin1"))
rhonyc_txt
##gfg corpus
gfg.srt <- file.path("../data/script/gfg")
gfg_txt <- VCorpus(DirSource(gfg.srt,encoding ="latin1"))
gfg_txt
##yshr
yshr.srt <- file.path("../data/script/yshr")
yshr_txt <- VCorpus(DirSource(yshr.srt,encoding ="latin1"))
yshr_txt
